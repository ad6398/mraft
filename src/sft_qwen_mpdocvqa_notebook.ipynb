{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9207aece-7c50-44c4-a242-f9c22758b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.colqwen_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "from PIL import Image\n",
    "# System prompt for MPDocVQA\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a vision-language assistant specialized in answering questions based on document page images.\n",
    "Given a question about the document, use the provided page images to only generate accurate, short and concise answers.\n",
    "\"\"\"\n",
    "\n",
    "def load_candidates(cands_path: str) -> dict:\n",
    "    with open(cands_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff5b160-46e9-4447-8696-592cbcd1e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example: dict, image_dir: str, candidates: dict, top_k: int) -> dict:\n",
    "    # Extract QID, question, and answer\n",
    "    qid = example['questionId']\n",
    "    question = example['question']\n",
    "    answers = example.get('answers', [])\n",
    "    answer = answers[0] if answers else \"\"\n",
    "\n",
    "    # Construct chat messages\n",
    "\n",
    "    # Select top-k candidate page IDs\n",
    "    cand_pages = candidates.get(str(qid), [])[:top_k]\n",
    "    # print(cand_pages, qid)\n",
    "    image_paths = [os.path.join(image_dir, 'images', f\"{pid}.jpg\") for pid, _ in cand_pages]\n",
    "    # print(image_paths)\n",
    "    user_messages_content = [\n",
    "        {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": question\n",
    "        }\n",
    "    ]\n",
    "    for image_path in image_paths:\n",
    "        user_messages_content.append(\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\",    \"content\": {\"type\": \"text\", \"text\": SYSTEM_MESSAGE}},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_messages_content\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": {\"type\": \"text\", \"text\": answer}},\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    # Separate raw multimodal content and chat msgs\n",
    "    # print(batch)\n",
    "    messages = [msg[\"messages\"] for msg in batch]\n",
    "    texts = [\n",
    "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "        for msg in messages\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Process vision inputs\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Tokenize text + align with vision features\n",
    "    batch_enc = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    labels = batch_enc['input_ids'].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100 # Mask padding tokens in labels\n",
    "    image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "    \n",
    "    batch_enc[\"labels\"] = labels\n",
    "\n",
    "\n",
    "    return batch_enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d78b796-313d-4521-bbdb-aaf18d72cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b0abf2-b605-4841-b2e9-2ba0c1d24a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SFTArgs:\n",
    "    train_json: str = \"mpdocvqa/question_answers/val.json\"\n",
    "    candidates_json: str = \"close_vanilla_colqwen_val_eval_4.json\"\n",
    "    root_dir: str= \"mpdocvqa\"\n",
    "    eval_json  = None\n",
    "    model_id: str = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    output_dir: str = \"./qwen2vl-sft-mpdocvqa\"\n",
    "    num_epochs: int = 1\n",
    "    batch_size: int = 1\n",
    "    lr: float = 2e-4\n",
    "    top_k: int = 1\n",
    "args = SFTArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43bc1539-7c58-4710-962c-6702e1d252fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['questionId', 'question', 'doc_id', 'page_ids', 'answers', 'answer_page_idx', 'data_split'],\n",
      "    num_rows: 5187\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_train = load_dataset('json', data_files={'train': args.train_json}, field='data')['train']\n",
    "raw_eval  = load_dataset('json', data_files={'eval': args.eval_json}, field='data')['eval'] if args.eval_json else None\n",
    "print(raw_train)\n",
    "# Load candidate-page mapping\n",
    "# Load candidate mappings and prepare samples\n",
    "candidates = load_candidates(args.candidates_json)\n",
    "# print(candidates['49153'])\n",
    "train_samples = [format_data(ex, args.root_dir, candidates, args.top_k) for ex in raw_train]\n",
    "eval_samples  = [format_data(ex, args.root_dir, candidates, args.top_k) for ex in raw_eval] if raw_eval else None\n",
    "# # Load model and processor with 4-bit quantization\n",
    "# # bnb_config = BitsAndBytesConfig(\n",
    "# #     load_in_4bit=True,\n",
    "# #     bnb_4bit_use_double_quant=True,\n",
    "# #     bnb_4bit_quant_type='nf4',\n",
    "# #     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# # )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8417b9f9-511b-460d-9ebb-c96757cb8a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,046,272 || all params: 8,297,212,928 || trainable%: 0.0608\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    args.model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None\n",
    ")\n",
    "processor = Qwen2_5_VLProcessor.from_pretrained(args.model_id, use_fast=True)\n",
    "# Prepare model for k-bit training, then apply LoRA adapters\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj','v_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b4c0c7-ade3-4b0b-aeda-3d1ac82abb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mak11089\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20250429_181436-uhac03zp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ak11089/my-ms-thesis/runs/uhac03zp' target=\"_blank\">vocal-moon-12</a></strong> to <a href='https://wandb.ai/ak11089/my-ms-thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ak11089/my-ms-thesis' target=\"_blank\">https://wandb.ai/ak11089/my-ms-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ak11089/my-ms-thesis/runs/uhac03zp' target=\"_blank\">https://wandb.ai/ak11089/my-ms-thesis/runs/uhac03zp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"my-ms-thesis\")\n",
    "wandb.config.update(model.config.to_dict(), allow_val_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bf71ca-482d-48d5-a0cf-5eadb38eb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collate_fn.processor = processor\n",
    "# candidates = load_candidates(args.candidates_json)\n",
    "# train_ds = MPDocVQADataset(args.train_json, args.root_dir, candidates, args.top_k, processor)\n",
    "# eval_ds  = MPDocVQADataset(args.eval_json, args.root_dir, candidates, args.top_k, processor) if args.eval_json else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056a9846-7832-45c9-8b73-f1c67ad5a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5e0461-ea9d-4ee6-b8d7-c48ff869f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SFT training\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=args.output_dir,\n",
    "    num_train_epochs=args.num_epochs,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    gradient_checkpointing=True,\n",
    "    # gradient_checkpoint=True,\n",
    "    optim='adamw_torch_fused',\n",
    "    learning_rate=args.lr,\n",
    "    lr_scheduler_type='constant',\n",
    "    logging_steps=1,\n",
    "    # eval_steps=50,\n",
    "    # eval_strategy='steps' if eval_samples else 'no',\n",
    "    report_to=['wandb'],\n",
    "    label_names = 'labels',\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field='messages',\n",
    "    dataset_kwargs={'skip_prepare_dataset': True},\n",
    "    bf16=True\n",
    ")\n",
    "# Initialize SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_samples,\n",
    "    eval_dataset=eval_samples,\n",
    "    data_collator=lambda exs: collate_fn(exs, processor),\n",
    "    peft_config=peft_config,\n",
    "    # tokenizer=processor.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26825c-0f30-402a-b88c-79d5c9fc62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/648 02:38 < 3:30:48, 0.05 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.764100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "# Save adapters and tokenizer\n",
    "# os.makedirs(args.output_dir, exist_ok=True)\n",
    "# model.save_pretrained(args.output_dir)\n",
    "# processor.tokenizer.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415c3d5-0f87-42c7-a6b2-7a868bf0fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# odel.save_pretrained(args.output_dir)\n",
    "processor.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878326ce-f0fb-4724-b9fb-ecfc3eccf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f9eb1-d535-4123-a995-514d16d52b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf args.output_dir "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
